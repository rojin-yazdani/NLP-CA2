{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for the IMDb Movie Reviews dataset     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BoW (Bag-of-Words) technique based on ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews, stopwords\n",
    "import re,string\n",
    "from bs4 import BeautifulSoup\n",
    "from random import shuffle \n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from nltk import classify, ngrams\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "from nltk.metrics.scores import precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus has 2000 reviews, with 1000 having the 'pos' label and the remaining 1000 having the 'neg' label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 2000\n",
      "Review categories: ['neg', 'pos']\n",
      "Total positive reviews: 1000\n",
      "Total negative reviews: 1000\n"
     ]
    }
   ],
   "source": [
    "print (\"Total reviews:\", len(movie_reviews.fileids())) \n",
    "print (\"Review categories:\", movie_reviews.categories()) \n",
    "print (\"Total positive reviews:\", len(movie_reviews.fileids('pos'))) \n",
    "print (\"Total negative reviews:\", len(movie_reviews.fileids('neg'))) \n",
    "positive_review_file = movie_reviews.fileids('neg')[1] \n",
    "#print (\"Sample review file:\", positive_review_file) \n",
    "#print(\"\\nContent of this sample review is :\", movie_reviews.raw(positive_review_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below accepts a movie review as an input argument and returns tokens as an output.<br>\n",
    "Different parts of preprocessing step are: <br>\n",
    "- removing html tags\n",
    "- removing sentences between [] or () that were extra explanation\n",
    "- removing punctuation marks\n",
    "- removing stop words\n",
    "- extracting lemma and replace the main words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # removing the html strips\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    # removing the square [] , {} , () , «»\n",
    "    text = re.sub('\\[[^]]*\\]', ' ', text)\n",
    "    text = re.sub('\\([^]]*\\)', ' ', text)\n",
    "    text = re.sub('\\«[^]]*\\»', ' ', text)\n",
    "    text = re.sub('\\{[^]]*\\}', ' ', text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove the punctuations\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Lower the tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, unigram tokensand bigram tokens, regardless of their places, are assuemd as features.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(words, ngram_words, forward = True):\n",
    "    new_text = []\n",
    "    index = 0\n",
    "    if not forward:\n",
    "        words = list(reversed(words))\n",
    "    while index < len(words):\n",
    "        [new_word, new_index] = concatenate_words(index, words, ngram_words, forward)\n",
    "        new_text.append(new_word)\n",
    "        index = new_index+1 if index != new_index else index+1\n",
    "    if not forward:\n",
    "        return list(reversed(new_text))\n",
    "    return new_text\n",
    " \n",
    "def concatenate_words(index, text, ngram_words, forward):\n",
    "    words = text[index]\n",
    "    if index == len(text)-1:\n",
    "        return words, index\n",
    "    if words.split(' ')[0] in ngram_words:\n",
    "        [new_word, new_index] = concatenate_words(index+1, text, ngram_words, forward)\n",
    "        if forward:\n",
    "            words = words + ' ' + new_word\n",
    "        else:\n",
    "            words = new_word + ' ' + words\n",
    "        index = new_index\n",
    "    return words, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "linking_words = ['and', 'any', 'anyone', 'anything', 'are', 'be', 'best', 'can', 'cannot', 'cant',\n",
    "                 \"can't\", 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'done',\n",
    "                 \"don't\", 'either', 'else', 'even', 'every', 'for', 'from', 'have', \"haven't\",\n",
    "                 \"he's\", 'is', \"isn't\", 'it', 'its', \"i've\", 'just', 'like', 'lots', 'many', 'maybe',\n",
    "                 'me', 'might', 'more', 'must', 'my', 'never', 'no', 'none', 'not', 'nothing', 'now',\n",
    "                 'of', 'on', 'once', 'one', 'only', 'or', 'overly', 'perfectly', 'perhaps',\n",
    "                 'probably', 'seemed', 'seems', \"she's\", 'should', 'simply', 'so', 'some',\n",
    "                 'somehow', 'something', 'soon', 'start', 'takes', 'tell', 'thank', \"that's\",\n",
    "                 'the', 'their', 'them', 'then', 'there', \"there's\", 'they', \"they're\", 'this',\n",
    "                 'those', 'to', 'too', 'totally', 'tried', 'truly', 'try', 'turns', 'until',\n",
    "                 'upon', 'use', 'very', 'wait', 'was', 'well', 'went', 'were', 'whether', 'which',\n",
    "                 'whole', 'why', 'will', 'wish', \"won't\", 'would',\"wouldn't\", 'you', \"you'll\",\n",
    "                 'your', \"you're\", 'yourself']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', \"wasn't\", 'good']\n",
      "['film', \"wasn't\", 'good']\n",
      "['film', 'was so good', 'and interesting']\n",
      "['film was so', 'good and', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "print(generate_ngrams(['film', 'wasn\\'t', 'good'], linking_words))\n",
    "print(generate_ngrams(['film', 'wasn\\'t', 'good'], linking_words, False))\n",
    "print(generate_ngrams(['film', 'was', 'so','good','and','interesting'], linking_words))\n",
    "print(generate_ngrams(['film', 'was', 'so','good','and','interesting'], linking_words,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    fileid_words_clean = clean_text(movie_reviews.raw(fileid))\n",
    "    pos_reviews.append(fileid_words_clean)\n",
    "\n",
    "neg_reviews = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    fileid_words_clean = clean_text(movie_reviews.raw(fileid))\n",
    "    neg_reviews.append(fileid_words_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = []\n",
    "for words in pos_reviews:\n",
    "    pos_features.append((dict([word, True] for word in words), 'pos'))\n",
    "    pos_features.append((dict([word, True] for word in generate_ngrams(words, linking_words)), 'pos'))\n",
    "    pos_features.append((dict([word, True] for word in generate_ngrams(words, linking_words, forward = False)), 'pos'))\n",
    "\n",
    "neg_features = []\n",
    "for words in neg_reviews:\n",
    "    neg_features.append((dict([word, True] for word in words), 'neg'))\n",
    "    neg_features.append((dict([word, True] for word in generate_ngrams(words, linking_words)), 'neg'))\n",
    "    neg_features.append((dict([word, True] for word in generate_ngrams(words, linking_words, forward = False)), 'neg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 1600 words were selected for training, while the final 400 words were chosen for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400 600\n"
     ]
    }
   ],
   "source": [
    "all_features = pos_features + neg_features\n",
    "shuffle(all_features)\n",
    "\n",
    "X_all = [list(item[0]) for item in all_features]\n",
    "y_all = [item[1] for item in all_features]\n",
    "\n",
    "X_train = X_all[:5400]\n",
    "y_train = y_all[:5400]\n",
    "X_test = X_all[5400:]\n",
    "y_test = y_all[5400:]\n",
    "\n",
    "#X_train = X_all[:3600]\n",
    "#y_train = y_all[:3600]\n",
    "#X_test = X_all[3600:]\n",
    "#y_test = y_all[3600:]\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique for dividing data into test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_generator(X, y, k_fold):\n",
    "    subset_size = int(len(X) / k_fold)\n",
    "    #print(\"subset_size:\", subset_size)\n",
    "    for k in range(k_fold):\n",
    "        X_train = X[:k * subset_size] + X[(k + 1) * subset_size:]\n",
    "        X_valid = X[k * subset_size:][:subset_size]\n",
    "        y_train = y[:k * subset_size] + y[(k + 1) * subset_size:]\n",
    "        y_valid = y[k * subset_size:][:subset_size]\n",
    "\n",
    "        yield X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NaiveBayes model was called five times for various portions of the data, with fold=5 being assumed as the input argument for the above technique. <br> In the end, the accuracy was determined by averaging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration :  1\n",
      "Accuracy: 0.9277777777777778\n",
      "\n",
      "Iteration :  2\n",
      "Accuracy: 0.9287037037037037\n",
      "\n",
      "Iteration :  3\n",
      "Accuracy: 0.9416666666666667\n",
      "\n",
      "Iteration :  4\n",
      "Accuracy: 0.925\n",
      "\n",
      "Iteration :  5\n",
      "Accuracy: 0.9296296296296296\n",
      "\n",
      "Average Accuracy :  0.9305555555555556\n"
     ]
    }
   ],
   "source": [
    "k_fold=5\n",
    "counter=0\n",
    "sumAccuracy = 0\n",
    "for X_fold_train, y_fold_train, X_fold_valid, y_fold_valid in k_fold_generator(X_train, y_train, k_fold):\n",
    "    #print(len(X_fold_train), len(y_fold_train), len(X_fold_valid), len(y_fold_valid))\n",
    "    counter += 1\n",
    "    print(\"\\nIteration : \", counter)\n",
    "    \n",
    "    train_features = []\n",
    "    for i in range(len(X_fold_train)):\n",
    "        train_features.append((dict([word, True] for word in X_fold_train[i]), y_fold_train[i]))\n",
    "        \n",
    "    classifier = NaiveBayesClassifier.train(train_features) \n",
    "    classifier.train\n",
    "    \n",
    "    test_features = []\n",
    "    for i in range(len(X_fold_valid)):\n",
    "        test_features.append((dict([word, True] for word in X_fold_valid[i]), y_fold_valid[i]))  \n",
    "            \n",
    "    accuracy = classify.accuracy(classifier, test_features)\n",
    "    sumAccuracy += accuracy\n",
    "    print(\"Accuracy:\", accuracy) \n",
    "\n",
    "print(\"\\nAverage Accuracy : \", sumAccuracy/k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 features with the most effect were chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   faced = True              pos : neg    =     21.8 : 1.0\n",
      "             magnificent = True              pos : neg    =     20.5 : 1.0\n",
      "              uninspired = True              neg : pos    =     18.1 : 1.0\n",
      "                 italian = True              pos : neg    =     17.9 : 1.0\n",
      "           unintentional = True              neg : pos    =     16.0 : 1.0\n",
      "                    skip = True              neg : pos    =     14.7 : 1.0\n",
      "              incoherent = True              neg : pos    =     14.0 : 1.0\n",
      "                  tucker = True              pos : neg    =     13.4 : 1.0\n",
      "                  rehash = True              neg : pos    =     13.3 : 1.0\n",
      "               stretched = True              neg : pos    =     13.3 : 1.0\n",
      "                   inane = True              neg : pos    =     13.3 : 1.0\n",
      "                    bore = True              neg : pos    =     13.0 : 1.0\n",
      "                 breasts = True              neg : pos    =     12.9 : 1.0\n",
      "                  denzel = True              pos : neg    =     12.7 : 1.0\n",
      "                    bont = True              neg : pos    =     12.6 : 1.0\n",
      "             devastating = True              pos : neg    =     12.1 : 1.0\n",
      "            breakthrough = True              pos : neg    =     12.1 : 1.0\n",
      "             captivating = True              pos : neg    =     12.1 : 1.0\n",
      "                  justin = True              neg : pos    =     11.9 : 1.0\n",
      "                  brooks = True              pos : neg    =     11.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (classifier.show_most_informative_features(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation was done using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos Precision: 0.8662420382165605\n",
      "pos Recall: 0.9645390070921985\n",
      "pos F1-Score: 0.9127516778523489\n",
      "neg Precision: 0.965034965034965\n",
      "neg Recall: 0.8679245283018868\n",
      "neg F1-Score: 0.9139072847682119\n",
      "Accuracy: 0.9133333333333333\n"
     ]
    }
   ],
   "source": [
    "test_features = []\n",
    "for i in range(len(X_test)):\n",
    "    test_features.append((dict([word, True] for word in X_test[i]), y_test[i]))\n",
    "         \n",
    "y_pred, gold_labels = defaultdict(set), defaultdict(set)\n",
    "    \n",
    "for i, (features, label) in enumerate(test_features):\n",
    "    y_pred[classifier.classify(features)].add(i)\n",
    "    gold_labels[label].add(i) \n",
    "\n",
    "for label in y_pred:\n",
    "    print(label, 'Precision:', precision(gold_labels[label], y_pred[label]))\n",
    "    print(label, 'Recall:', recall(gold_labels[label], y_pred[label]))\n",
    "    print(label, 'F1-Score:', f_measure(gold_labels[label], y_pred[label]))\n",
    "accuracy = classify.accuracy(classifier, test_features)\n",
    "        \n",
    "print(\"Accuracy:\", accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
