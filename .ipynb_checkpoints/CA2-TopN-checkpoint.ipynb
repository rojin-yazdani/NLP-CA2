{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for the IMDb Movie Reviews dataset         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using top n frequent words as model features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Roghi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Roghi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import re,string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from random import shuffle \n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from nltk import classify, NaiveBayesClassifier\n",
    "\n",
    "from nltk.metrics.scores import precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus has 2000 reviews, with 1000 having the 'pos' label and the remaining 1000 having the 'neg' label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 2000\n",
      "Review categories: ['neg', 'pos']\n",
      "Total positive reviews: 1000\n",
      "Total negative reviews: 1000\n",
      "Sample review file: neg/cv001_19502.txt\n"
     ]
    }
   ],
   "source": [
    "print (\"Total reviews:\", len(movie_reviews.fileids())) \n",
    "print (\"Review categories:\", movie_reviews.categories()) \n",
    "print (\"Total positive reviews:\", len(movie_reviews.fileids('pos'))) \n",
    "print (\"Total negative reviews:\", len(movie_reviews.fileids('neg'))) \n",
    "positive_review_file = movie_reviews.fileids('neg')[1] \n",
    "print (\"Sample review file:\", positive_review_file) \n",
    "#print(\"\\nContent of this sample review is :\", movie_reviews.raw(positive_review_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below accepts a movie review as an input argument and returns tokens as an output.<br>\n",
    "Different parts of preprocessing step are: <br>\n",
    "- removing html tags\n",
    "- removing sentences between [] or () that were extra explanation\n",
    "- removing punctuation marks\n",
    "- removing stop words\n",
    "- extracting lemma and replace the main words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # removing the html strips\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    # removing the square [] , {} , () , «»\n",
    "    text = re.sub('\\[[^]]*\\]', ' ', text)\n",
    "    text = re.sub('\\([^]]*\\)', ' ', text)\n",
    "    text = re.sub('\\«[^]]*\\»', ' ', text)\n",
    "    text = re.sub('\\{[^]]*\\}', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove the punctuations\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Lower the tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove stopword\n",
    "    tokens = [word for word in tokens if not word in stopwords.words(\"english\")]\n",
    "    # Lemmatize\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = [lemma.lemmatize(word, pos = \"v\") for word in tokens]\n",
    "    tokens = [lemma.lemmatize(word, pos = \"n\") for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each movie review, preprocessing was done and separated positive reviews and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of reviews: 2000\n",
      "Num of all_pos_words_clean: 156252\n",
      "Num of all_neg_words_clean: 144619\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "all_pos_words_clean = []\n",
    "all_neg_words_clean = []\n",
    "\n",
    "num=1\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        if num<=2000:\n",
    "            num+=1\n",
    "            fileid_words_clean = clean_text(movie_reviews.raw(fileid))\n",
    "            for word in fileid_words_clean:\n",
    "                if category=='pos':\n",
    "                    all_pos_words_clean.append(word)\n",
    "                else:\n",
    "                    all_neg_words_clean.append(word)\n",
    "                \n",
    "            reviews.append((fileid_words_clean, category))\n",
    "\n",
    "print(\"Num of reviews:\", len(reviews)) \n",
    "print(\"Num of all_pos_words_clean:\", len(all_pos_words_clean)) \n",
    "print(\"Num of all_neg_words_clean:\", len(all_neg_words_clean)) \n",
    "\n",
    "#print (\"Sample raw review:\\n\", movie_reviews.raw(review_fileids[10]))\n",
    "#print (\"\\nSample tokenized review after preprocessing:\\n\", reviews[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, words frequency was calculated for all positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of All pos words: 156252\n",
      "Length of All pos unique words: 14803\n",
      "----------------------------------------------------\n",
      "Length of All neg words: 144619\n",
      "Length of All neg unique words: 13930\n",
      "----------------------------------------------------\n",
      "\n",
      "Top 20 most frequently occurring POS words:\n",
      " [('film', 3026), ('movie', 1828), ('one', 1395), ('make', 1031), ('character', 874), ('see', 864), ('like', 826), ('time', 713), ('get', 709), ('story', 651), ('go', 647), ('good', 587), ('scene', 550), ('even', 532), ('take', 511), ('play', 505), ('end', 495), ('would', 489), ('well', 479), ('come', 460)]\n",
      "\n",
      "Top 20 most frequently occurring NEG words:\n",
      " [('film', 2656), ('movie', 2201), ('one', 1339), ('make', 1065), ('like', 969), ('get', 866), ('character', 822), ('even', 715), ('go', 706), ('see', 699), ('time', 679), ('would', 644), ('good', 595), ('story', 557), ('bad', 540), ('scene', 523), ('much', 500), ('look', 488), ('play', 485), ('could', 485)]\n"
     ]
    }
   ],
   "source": [
    "all_pos_words_frequency = FreqDist(all_pos_words_clean)\n",
    "all_neg_words_frequency = FreqDist(all_neg_words_clean)\n",
    "\n",
    "print(\"Length of All pos words:\", len(all_pos_words_clean))\n",
    "print(\"Length of All pos unique words:\", len(all_pos_words_frequency))\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Length of All neg words:\", len(all_neg_words_clean))\n",
    "print(\"Length of All neg unique words:\", len(all_neg_words_frequency))\n",
    "print(\"----------------------------------------------------\")\n",
    "print (\"\\nTop 20 most frequently occurring POS words:\\n\", all_pos_words_frequency.most_common(20))\n",
    "print (\"\\nTop 20 most frequently occurring NEG words:\\n\", all_neg_words_frequency.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each label, the most 500 frequent words, were extracted and removed from all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_freq = 500\n",
    "pos_most_freq_words = []\n",
    "neg_most_freq_words = []\n",
    "\n",
    "for word_freq in all_pos_words_frequency.most_common(num_freq):\n",
    "    pos_most_freq_words.append(word_freq[0])\n",
    "    \n",
    "for word_freq in all_neg_words_frequency.most_common(num_freq):\n",
    "    neg_most_freq_words.append(word_freq[0])   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79143\n",
      "71172\n"
     ]
    }
   ],
   "source": [
    "#pos_words = set(all_pos_words_clean).difference(set(all_neg_words_clean))\n",
    "#neg_words = set(all_neg_words_clean).difference(set(all_pos_words_clean))\n",
    "pos_words = [word for word in all_pos_words_clean if not word in neg_most_freq_words]\n",
    "neg_words = [word for word in all_neg_words_clean if not word in pos_most_freq_words]\n",
    "\n",
    "print(len(pos_words))\n",
    "print(len(neg_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, the top 1000 words for each label were retrieved and utilized as the models' features for the remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of All pos words: 79143\n",
      "Length of All pos unique words: 14303\n",
      "----------------------------------------------------\n",
      "Length of All neg words: 71172\n",
      "Length of All neg unique words: 13430\n",
      "----------------------------------------------------\n",
      "\n",
      "Top 20 most frequently occurring POS words:\n",
      " [('mother', 104), ('joe', 103), ('beautiful', 102), ('fiction', 100), ('strong', 92), ('fine', 90), ('touch', 89), ('dream', 88), ('son', 88), ('flaw', 88), ('tale', 87), ('others', 85), ('person', 84), ('art', 84), ('remain', 82), ('ship', 81), ('voice', 81), ('science', 80), ('throughout', 79), ('image', 78)]\n",
      "\n",
      "Top 20 most frequently occurring NEG words:\n",
      " [('bore', 167), ('worst', 143), ('waste', 139), ('unfortunately', 122), ('maybe', 118), ('suppose', 116), ('tv', 113), ('guess', 90), ('stupid', 88), ('career', 85), ('car', 84), ('worse', 84), ('batman', 83), ('wild', 80), ('mess', 79), ('cop', 77), ('terrible', 76), ('none', 76), ('obvious', 76), ('early', 74)]\n"
     ]
    }
   ],
   "source": [
    "sel_pos_words_frequency = FreqDist(pos_words)\n",
    "sel_neg_words_frequency = FreqDist(neg_words)\n",
    "\n",
    "print(\"Length of All pos words:\", len(pos_words))\n",
    "print(\"Length of All pos unique words:\", len(sel_pos_words_frequency))\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Length of All neg words:\", len(neg_words))\n",
    "print(\"Length of All neg unique words:\", len(sel_neg_words_frequency))\n",
    "print(\"----------------------------------------------------\")\n",
    "print (\"\\nTop 20 most frequently occurring POS words:\\n\", sel_pos_words_frequency.most_common(20))\n",
    "print (\"\\nTop 20 most frequently occurring NEG words:\\n\", sel_neg_words_frequency.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frequently = 1000\n",
    "most_common_words = sel_neg_words_frequency.most_common(num_frequently) + sel_pos_words_frequency.most_common(num_frequently)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In method below, tuples were created for these 2000 frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_features(review_words, word_features):    \n",
    "    review_unique_words = set(review_words)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in review_unique_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = [item[0] for item in most_common_words]\n",
    "\n",
    "feature_set = [(review_features(doc, word_features), category) for (doc, category) in reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document: (['always', 'careful', 'first', 'official', 'studio', 'release', 'gate', 'year', 'obviously', 'film', 'studio', 'great', 'hop', 'however', 'month', 'picture', 'movie', 'like', 'firestorm', 'least', 'give', 'perspective', 'good', 'movie', 'really', 'fact', 'good', 'chance', 'still', 'play', 'near'], 'neg')\n",
      "\n",
      "Num of features of Sample document: 31\n",
      "\n",
      "Num of new features: 1411\n"
     ]
    }
   ],
   "source": [
    "print (\"Sample document:\", reviews[500])\n",
    "print (\"\\nNum of features of Sample document:\", len(reviews[500][0]))\n",
    "\n",
    "sample_doc_new_features = review_features(reviews[500][0],word_features)\n",
    "print(\"\\nNum of new features:\", len(sample_doc_new_features))\n",
    "#print(\"\\nSample document new features:\\n\", sample_doc_new_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 1600 words were selected for training, while the final 400 words were chosen for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 1600 \n",
      "Test Size: 400\n"
     ]
    }
   ],
   "source": [
    "shuffle(feature_set)\n",
    "\n",
    "X_all = [list(item[0]) for item in feature_set]\n",
    "y_all = [item[1] for item in feature_set]\n",
    "\n",
    "X_train = X_all[:1600]\n",
    "y_train = y_all[:1600]\n",
    "X_test = X_all[1600:]\n",
    "y_test = y_all[1600:]\n",
    "print(\"Train Size:\", len(X_train), \"\\nTest Size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique for dividing data into test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_generator(X, y, k_fold):\n",
    "    subset_size = int(len(X) / k_fold)\n",
    "    #print(\"subset_size:\", subset_size)\n",
    "    for k in range(k_fold):\n",
    "        X_train = X[:k * subset_size] + X[(k + 1) * subset_size:]\n",
    "        X_valid = X[k * subset_size:][:subset_size]\n",
    "        y_train = y[:k * subset_size] + y[(k + 1) * subset_size:]\n",
    "        y_valid = y[k * subset_size:][:subset_size]\n",
    "\n",
    "        yield X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NaiveBayes model was called five times for various portions of the data, with fold=5 being assumed as the input argument for the above technique. <br> In the end, the accuracy was determined by averaging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration :  1\n",
      "Accuracy: 0.478125\n",
      "\n",
      "Iteration :  2\n",
      "Accuracy: 0.496875\n",
      "\n",
      "Iteration :  3\n",
      "Accuracy: 0.44375\n",
      "\n",
      "Iteration :  4\n",
      "Accuracy: 0.4875\n",
      "\n",
      "Iteration :  5\n",
      "Accuracy: 0.503125\n",
      "\n",
      "Average Accuracy :  0.48187500000000005\n"
     ]
    }
   ],
   "source": [
    "k_fold=5\n",
    "counter=0\n",
    "sumAccuracy = 0\n",
    "for X_fold_train, y_fold_train, X_fold_valid, y_fold_valid in k_fold_generator(X_train, y_train, k_fold):\n",
    "    #print(len(X_fold_train), len(y_fold_train), len(X_fold_valid), len(y_fold_valid))\n",
    "    counter += 1\n",
    "    print(\"\\nIteration : \", counter)\n",
    "    \n",
    "    train_features = []\n",
    "    for i in range(len(X_fold_train)):\n",
    "        train_features.append((dict([word, True] for word in X_fold_train[i]), y_fold_train[i]))\n",
    "        \n",
    "    classifier = NaiveBayesClassifier.train(train_features) \n",
    "    \n",
    "    test_features = []\n",
    "    for i in range(len(X_fold_valid)):\n",
    "        test_features.append((dict([word, True] for word in X_fold_valid[i]), y_fold_valid[i]))  \n",
    "            \n",
    "    accuracy = classify.accuracy(classifier, test_features)\n",
    "    sumAccuracy += accuracy\n",
    "    print(\"Accuracy:\", accuracy) \n",
    "\n",
    "print(\"\\nAverage Accuracy : \", sumAccuracy/k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 features with the most effect were chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        contains(stupid) = True              neg : pos    =      1.0 : 1.0\n",
      "           contains(buy) = True              neg : pos    =      1.0 : 1.0\n",
      "           contains(gay) = True              neg : pos    =      1.0 : 1.0\n",
      "        contains(affair) = True              neg : pos    =      1.0 : 1.0\n",
      "          contains(slow) = True              neg : pos    =      1.0 : 1.0\n",
      "           contains(fox) = True              neg : pos    =      1.0 : 1.0\n",
      "          contains(band) = True              neg : pos    =      1.0 : 1.0\n",
      "           contains(joe) = True              neg : pos    =      1.0 : 1.0\n",
      "        contains(gibson) = True              neg : pos    =      1.0 : 1.0\n",
      "          contains(shin) = True              neg : pos    =      1.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation was done using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4825\n"
     ]
    }
   ],
   "source": [
    "test_features = []\n",
    "for i in range(len(X_test)):\n",
    "    test_features.append((dict([word, True] for word in X_test[i]), y_test[i]))\n",
    "         \n",
    "accuracy = classify.accuracy(classifier, test_features)\n",
    "        \n",
    "print(\"Accuracy:\", accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
