{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for the IMDb Movie Reviews dataset     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BOW technique based on bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import re,string\n",
    "from bs4 import BeautifulSoup\n",
    "from random import shuffle \n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from nltk import classify, ngrams, NaiveBayesClassifier\n",
    "\n",
    "from nltk.metrics.scores import precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus has 2000 reviews, with 1000 having the 'pos' label and the remaining 1000 having the 'neg' label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 2000\n",
      "Review categories: ['neg', 'pos']\n",
      "Total positive reviews: 1000\n",
      "Total negative reviews: 1000\n",
      "Sample review file: neg/cv001_19502.txt\n"
     ]
    }
   ],
   "source": [
    "print (\"Total reviews:\", len(movie_reviews.fileids())) \n",
    "print (\"Review categories:\", movie_reviews.categories()) \n",
    "print (\"Total positive reviews:\", len(movie_reviews.fileids('pos'))) \n",
    "print (\"Total negative reviews:\", len(movie_reviews.fileids('neg'))) \n",
    "positive_review_file = movie_reviews.fileids('neg')[1] \n",
    "print (\"Sample review file:\", positive_review_file) \n",
    "#print(\"\\nContent of this sample review is :\", movie_reviews.raw(positive_review_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below accepts a movie review as an input argument and returns tokens as an output.<br>\n",
    "Different parts of preprocessing step are: <br>\n",
    "- removing html tags\n",
    "- removing sentences between [] or () that were extra explanation\n",
    "- removing punctuation marks\n",
    "- removing stop words\n",
    "- extracting lemma and replace the main words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "linking_words = ['above', 'below', 'off', 'over', 'under', 'more', 'most',\n",
    "                   'such', 'no', 'nor', 'not', 'only', 'so', 'than', 'too',\n",
    "                   'very', 'just', 'but']\n",
    "\n",
    "bigram_stop_words = set(stop_words) - set(linking_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stop_words):\n",
    "    # removing the html strips\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    # removing the square [] , {} , () , «»\n",
    "    text = re.sub('\\[[^]]*\\]', ' ', text)\n",
    "    text = re.sub('\\([^]]*\\)', ' ', text)\n",
    "    text = re.sub('\\«[^]]*\\»', ' ', text)\n",
    "    text = re.sub('\\{[^]]*\\}', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove the punctuations\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Lower the tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove stopword\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    # Lemmatize\n",
    "    #lemma = WordNetLemmatizer()\n",
    "    #tokens = [lemma.lemmatize(word, pos = \"v\") for word in tokens]\n",
    "    #tokens = [lemma.lemmatize(word, pos = \"n\") for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews = []\n",
    "bigram_pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    #extract unigram cleaned words\n",
    "    fileid_words_clean = clean_text(movie_reviews.raw(fileid),stop_words)\n",
    "    pos_reviews.append(fileid_words_clean)\n",
    "    #extract bigram cleaned words\n",
    "    fileid_words_clean = clean_text(movie_reviews.raw(fileid),bigram_stop_words)\n",
    "    bigram_pos_reviews.append(fileid_words_clean)\n",
    "\n",
    "neg_reviews = []\n",
    "bigram_neg_reviews = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    #extract unigram cleaned words\n",
    "    fileid_words_clean = clean_text(movie_reviews.raw(fileid),stop_words)\n",
    "    neg_reviews.append(fileid_words_clean)\n",
    "    #extract bigram cleaned words\n",
    "    fileid_words_clean = clean_text(movie_reviews.raw(fileid),bigram_stop_words)\n",
    "    bigram_neg_reviews.append(fileid_words_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOW of sample review:\n",
      " ['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'superheroes', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'half', 'bad', 'film', 'however', 'good', 'r', 'strong', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "#print(\"Raw text of sample review:\\n\", (movie_reviews.raw(pos_review_fileids[0])))\n",
    "print(\"\\nBOW of sample review:\\n\", (pos_reviews[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, unigram tokensand bigram tokens, regardless of their places, are assuemd as features.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(words):\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_ngrams(words, n=2):\n",
    "    words_ng = []\n",
    "    for item in iter(ngrams(words, n)):\n",
    "        words_ng.append(item)\n",
    "    words_dictionary = dict([word, True] for word in words_ng)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews_set = []\n",
    "for i in range(len(pos_reviews)):\n",
    "    unigram_features = bag_of_words(pos_reviews[i])\n",
    "    bigram_features = bag_of_ngrams(bigram_pos_reviews[i])\n",
    "    all_features = unigram_features.copy()\n",
    "    all_features.update(bigram_features)\n",
    "    pos_reviews_set.append((all_features, 'pos'))\n",
    "\n",
    "neg_reviews_set = []\n",
    "for i in range(len(neg_reviews)):\n",
    "    unigram_features = bag_of_words(neg_reviews[i])\n",
    "    bigram_features = bag_of_ngrams(bigram_neg_reviews[i])\n",
    "    all_features = unigram_features.copy()\n",
    "    all_features.update(bigram_features)\n",
    "    neg_reviews_set.append((all_features, 'neg'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"\\nSample positive review:\\n\",pos_reviews_set[0])\n",
    "#print (\"\\nSample negative review:\\n\", neg_reviews_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 1600 words were selected for training, while the final 400 words were chosen for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 1600 \n",
      "Test Size: 400\n"
     ]
    }
   ],
   "source": [
    "all_features = pos_reviews_set + neg_reviews_set\n",
    "shuffle(all_features)\n",
    "\n",
    "X_all = [list(item[0]) for item in all_features]\n",
    "y_all = [item[1] for item in all_features]\n",
    "\n",
    "X_train = X_all[:1600]\n",
    "y_train = y_all[:1600]\n",
    "X_test = X_all[1600:]\n",
    "y_test = y_all[1600:]\n",
    "print(\"Train Size:\", len(X_train), \"\\nTest Size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique for dividing data into test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_generator(X, y, k_fold):\n",
    "    subset_size = int(len(X) / k_fold)\n",
    "    #print(\"subset_size:\", subset_size)\n",
    "    for k in range(k_fold):\n",
    "        X_train = X[:k * subset_size] + X[(k + 1) * subset_size:]\n",
    "        X_valid = X[k * subset_size:][:subset_size]\n",
    "        y_train = y[:k * subset_size] + y[(k + 1) * subset_size:]\n",
    "        y_valid = y[k * subset_size:][:subset_size]\n",
    "\n",
    "        yield X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NaiveBayes model was called five times for various portions of the data, with fold=5 being assumed as the input argument for the above technique. <br> In the end, the accuracy was determined by averaging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration :  1\n",
      "Accuracy: 0.80625\n",
      "\n",
      "Iteration :  2\n",
      "Accuracy: 0.796875\n",
      "\n",
      "Iteration :  3\n",
      "Accuracy: 0.734375\n",
      "\n",
      "Iteration :  4\n",
      "Accuracy: 0.815625\n",
      "\n",
      "Iteration :  5\n",
      "Accuracy: 0.790625\n",
      "\n",
      "Average Accuracy :  0.7887500000000001\n"
     ]
    }
   ],
   "source": [
    "k_fold=5\n",
    "counter=0\n",
    "sumAccuracy = 0\n",
    "for X_fold_train, y_fold_train, X_fold_valid, y_fold_valid in k_fold_generator(X_train, y_train, k_fold):\n",
    "    #print(len(X_fold_train), len(y_fold_train), len(X_fold_valid), len(y_fold_valid))\n",
    "    counter += 1\n",
    "    print(\"\\nIteration : \", counter)\n",
    "    \n",
    "    train_features = []\n",
    "    for i in range(len(X_fold_train)):\n",
    "        train_features.append((dict([word, True] for word in X_fold_train[i]), y_fold_train[i]))\n",
    "        \n",
    "    classifier = NaiveBayesClassifier.train(train_features) \n",
    "    classifier.train\n",
    "    \n",
    "    test_features = []\n",
    "    for i in range(len(X_fold_valid)):\n",
    "        test_features.append((dict([word, True] for word in X_fold_valid[i]), y_fold_valid[i]))  \n",
    "            \n",
    "    accuracy = classify.accuracy(classifier, test_features)\n",
    "    sumAccuracy += accuracy\n",
    "    print(\"Accuracy:\", accuracy) \n",
    "\n",
    "print(\"\\nAverage Accuracy : \", sumAccuracy/k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 features with the most effect were chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        ('one', 'worst') = True              neg : pos    =     17.3 : 1.0\n",
      "             wonderfully = True              pos : neg    =     12.9 : 1.0\n",
      "     ('batman', 'robin') = True              neg : pos    =     11.7 : 1.0\n",
      "                ordinary = True              pos : neg    =     10.4 : 1.0\n",
      "               redeeming = True              neg : pos    =      9.5 : 1.0\n",
      "             outstanding = True              pos : neg    =      8.9 : 1.0\n",
      "               fantastic = True              pos : neg    =      8.9 : 1.0\n",
      "                  arnold = True              neg : pos    =      8.8 : 1.0\n",
      "       ('waste', 'time') = True              neg : pos    =      8.7 : 1.0\n",
      "                horribly = True              neg : pos    =      8.1 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation was done using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos Precision: 0.7198275862068966\n",
      "pos Recall: 0.8434343434343434\n",
      "pos F1-Score: 0.7767441860465116\n",
      "neg Precision: 0.8154761904761905\n",
      "neg Recall: 0.6782178217821783\n",
      "neg F1-Score: 0.7405405405405405\n",
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "test_features = []\n",
    "for i in range(len(X_test)):\n",
    "    test_features.append((dict([word, True] for word in X_test[i]), y_test[i]))\n",
    "         \n",
    "y_pred, gold_labels = defaultdict(set), defaultdict(set)\n",
    "    \n",
    "for i, (features, label) in enumerate(test_features):\n",
    "    y_pred[classifier.classify(features)].add(i)\n",
    "    gold_labels[label].add(i) \n",
    "\n",
    "for label in y_pred:\n",
    "    print(label, 'Precision:', precision(gold_labels[label], y_pred[label]))\n",
    "    print(label, 'Recall:', recall(gold_labels[label], y_pred[label]))\n",
    "    print(label, 'F1-Score:', f_measure(gold_labels[label], y_pred[label]))\n",
    "accuracy = classify.accuracy(classifier, test_features)\n",
    "        \n",
    "print(\"Accuracy:\", accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
